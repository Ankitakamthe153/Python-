{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a dataset which classified if patients have heart disease or not according to feature in it.Here,I will try to use this data to create a model which tries predict if a patient has this disease or not. I will use logistic regression(classification) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data contains;\n",
    "\n",
    "age - age in years\n",
    "sex - (1 = male; 0 = female)\n",
    "cp - chest pain type\n",
    "trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "chol - serum cholestoral in mg/dl\n",
    "fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "restecg - resting electrocardiographic results\n",
    "thalach - maximum heart rate achieved\n",
    "exang - exercise induced angina (1 = yes; 0 = no)\n",
    "oldpeak - ST depression induced by exercise relative to rest\n",
    "slope - the slope of the peak exercise ST segment\n",
    "ca - number of major vessels (0-3) colored by flourosopy\n",
    "thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "target - have disease or not (1=yes, 0=no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"target\", data=df, palette=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoDisease = len(df[df.target == 0])\n",
    "countHaveDisease = len(df[df.target == 1])\n",
    "print(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease / (len(df.target))*100)))\n",
    "print(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease / (len(df.target))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sex', data=df, palette=\"mako_r\")\n",
    "plt.xlabel(\"Sex (0 = female, 1= male)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countFemale = len(df[df.sex == 0])\n",
    "countMale = len(df[df.sex == 1])\n",
    "print(\"Percentage of Female Patients: {:.2f}%\".format((countFemale / (len(df.sex))*100)))\n",
    "print(\"Percentage of Male Patients: {:.2f}%\".format((countMale / (len(df.sex))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\n",
    "plt.title('Heart Disease Frequency for Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('heartDiseaseAndAges.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\n",
    "plt.title('Heart Disease Frequency for Sex')\n",
    "plt.xlabel('Sex (0 = Female, 1 = Male)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend([\"Haven't Disease\", \"Have Disease\"])\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\n",
    "plt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\n",
    "plt.legend([\"Disease\", \"Not Disease\"])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Maximum Heart Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.slope,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#DAF7A6','#FF5733' ])\n",
    "plt.title('Heart Disease Frequency for Slope')\n",
    "plt.xlabel('The Slope of The Peak Exercise ST Segment ')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.fbs,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#FFC300','#581845' ])\n",
    "plt.title('Heart Disease Frequency According To FBS')\n",
    "plt.xlabel('FBS - (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.legend([\"Haven't Disease\", \"Have Disease\"])\n",
    "plt.ylabel('Frequency of Disease or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#11A5AA','#AA1190' ])\n",
    "plt.title('Heart Disease Frequency According To Chest Pain Type')\n",
    "plt.xlabel('Chest Pain Type')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.ylabel('Frequency of Disease or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'cp','thal' and 'slope' are categorical variables I'll turn them into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.get_dummies(df['cp'], prefix = \"cp\")\n",
    "b = pd.get_dummies(df['thal'], prefix = \"thal\")\n",
    "c = pd.get_dummies(df['slope'], prefix = \"slope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df, a, b, c]\n",
    "df = pd.concat(frames, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['cp', 'thal', 'slope'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use sklearn library or I can write functions ourselves.Let's them both.Firstly I will write our function after that I'll use sklearn library to calculate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target.values\n",
    "x_data = df.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split our data. 80% of data will be train data and 20% of it will be test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose matrices\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say weight=0.01 and bias=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "def initialize(dimension):\n",
    "    \n",
    "    weight = np.full((dimension,1),0.01)\n",
    "    bias = 0.0\n",
    "    return weight,bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    y_head = 1/(1+ np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardBackward(weight,bias,x_train,y_train):\n",
    "    # Forward\n",
    "    \n",
    "    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n",
    "    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n",
    "    cost = np.sum(loss) / x_train.shape[1]\n",
    "    \n",
    "    # Backward\n",
    "    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n",
    "    \n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(weight,bias,x_train,y_train,learningRate,iteration) :\n",
    "    costList = []\n",
    "    index = []\n",
    "    \n",
    "    #for each iteration, update weight and bias values\n",
    "    for i in range(iteration):\n",
    "        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n",
    "        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n",
    "        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n",
    "        \n",
    "        costList.append(cost)\n",
    "        index.append(i)\n",
    "\n",
    "    parameters = {\"weight\": weight,\"bias\": bias}\n",
    "    \n",
    "    print(\"iteration:\",iteration)\n",
    "    print(\"cost:\",cost)\n",
    "\n",
    "    plt.plot(index,costList)\n",
    "    plt.xlabel(\"Number of Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "\n",
    "    return parameters, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weight,bias,x_test):\n",
    "    z = np.dot(weight.T,x_test) + bias\n",
    "    y_head = sigmoid(z)\n",
    "\n",
    "    y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    \n",
    "    for i in range(y_head.shape[1]):\n",
    "        if y_head[0,i] <= 0.5:\n",
    "            y_prediction[0,i] = 0\n",
    "        else:\n",
    "            y_prediction[0,i] = 1\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n",
    "    dimension = x_train.shape[0]\n",
    "    weight,bias = initialize(dimension)\n",
    "    \n",
    "    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n",
    "\n",
    "    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    \n",
    "    print(\"Manuel Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(x_train,y_train,x_test,y_test,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out sklearn's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train.T,y_train.T)\n",
    "acc = lr.score(x_test.T,y_test.T)*100\n",
    "\n",
    "accuracies['Logistic Regression'] = acc\n",
    "print(\"Test Accuracy {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model works with 86.89% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour(KNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what will be score if we use KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
    "knn.fit(x_train.T, y_train.T)\n",
    "prediction = knn.predict(x_test.T)\n",
    "\n",
    "print(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to find best K value\n",
    "scoreList = []\n",
    "for i in range(1,20):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "    knn2.fit(x_train.T, y_train.T)\n",
    "    scoreList.append(knn2.score(x_test.T, y_test.T))\n",
    "    \n",
    "plt.plot(range(1,20), scoreList)\n",
    "plt.xticks(np.arange(1,20,1))\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "acc = max(scoreList)*100\n",
    "accuracies['KNN'] = acc\n",
    "print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above if we define k as 3-7-8 we will reach maximum score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use SVM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state = 1)\n",
    "svm.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = svm.score(x_test.T,y_test.T)*100\n",
    "accuracies['SVM'] = acc\n",
    "print(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = nb.score(x_test.T,y_test.T)*100\n",
    "accuracies['Naive Bayes'] = acc\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = dtc.score(x_test.T, y_test.T)*100\n",
    "accuracies['Decision Tree'] = acc\n",
    "print(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n",
    "rf.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = rf.score(x_test.T,y_test.T)*100\n",
    "accuracies['Random Forest'] = acc\n",
    "print(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values\n",
    "y_head_lr = lr.predict(x_test.T)\n",
    "knn3 = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn3.fit(x_train.T, y_train.T)\n",
    "y_head_knn = knn3.predict(x_test.T)\n",
    "y_head_svm = svm.predict(x_test.T)\n",
    "y_head_nb = nb.predict(x_test.T)\n",
    "y_head_dtc = dtc.predict(x_test.T)\n",
    "y_head_rf = rf.predict(x_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_lr = confusion_matrix(y_test,y_head_lr)\n",
    "cm_knn = confusion_matrix(y_test,y_head_knn)\n",
    "cm_svm = confusion_matrix(y_test,y_head_svm)\n",
    "cm_nb = confusion_matrix(y_test,y_head_nb)\n",
    "cm_dtc = confusion_matrix(y_test,y_head_dtc)\n",
    "cm_rf = confusion_matrix(y_test,y_head_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\",fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "sns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"K Nearest Neighbors Confusion Matrix\")\n",
    "sns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Support Vector Machine Confusion Matrix\")\n",
    "sns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "sns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Decision Tree Classifier Confusion Matrix\")\n",
    "sns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "sns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
